{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SFvacJwaI8yP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (4.33.0)\n",
      "Requirement already satisfied: urllib3~=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.4.26 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2025.4.26)\n",
      "Requirement already satisfied: typing_extensions~=4.13.2 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Requirement already satisfied: scrapy in /opt/anaconda3/lib/python3.12/site-packages (2.11.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (42.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (24.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (69.5.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (23.2)\n",
      "Requirement already satisfied: tldextract in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: lxml>=4.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (5.2.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/anaconda3/lib/python3.12/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /opt/anaconda3/lib/python3.12/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
      "Requirement already satisfied: pyasn1-modules in /opt/anaconda3/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in /opt/anaconda3/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: automat>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (4.13.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tldextract->scrapy) (2.32.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yhDBnpOlJBq4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake_useragent in /opt/anaconda3/lib/python3.12/site-packages (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w-8pvimhJBt3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scrapy.selector import Selector\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9If_2tzzJBwx",
    "outputId": "a82768bb-78b4-427e-ed1c-7862fad215a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: Avengers: Endgame (2019) - User reviews - IMDb\n"
     ]
    }
   ],
   "source": [
    "#title\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "userAgent = ua.chrome\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument(f'user-agent={userAgent}')\n",
    "# Create a new instance of the WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Navigate to a website\n",
    "url = 'https://www.imdb.com/title/tt4154796/reviews?spoiler=hide&sort=curated&dir=desc&ratingFilter=0'\n",
    "driver.get(url)\n",
    "\n",
    "# Get the page title\n",
    "page_title = driver.title\n",
    "print(f\"Page Title: {page_title}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "G3x8UHHOJBzc"
   },
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n0   chromedriver                        0x00000001028d2654 cxxbridge1$str$ptr + 2723108\n1   chromedriver                        0x00000001028ca8c8 cxxbridge1$str$ptr + 2690968\n2   chromedriver                        0x000000010241e714 cxxbridge1$string$len + 90428\n3   chromedriver                        0x00000001024657c0 cxxbridge1$string$len + 381416\n4   chromedriver                        0x00000001024a6de8 cxxbridge1$string$len + 649232\n5   chromedriver                        0x00000001024599c8 cxxbridge1$string$len + 332784\n6   chromedriver                        0x0000000102896278 cxxbridge1$str$ptr + 2476360\n7   chromedriver                        0x000000010289950c cxxbridge1$str$ptr + 2489308\n8   chromedriver                        0x0000000102877a64 cxxbridge1$str$ptr + 2351412\n9   chromedriver                        0x0000000102899d94 cxxbridge1$str$ptr + 2491492\n10  chromedriver                        0x0000000102868d58 cxxbridge1$str$ptr + 2290728\n11  chromedriver                        0x00000001028b9d60 cxxbridge1$str$ptr + 2622512\n12  chromedriver                        0x00000001028b9eec cxxbridge1$str$ptr + 2622908\n13  chromedriver                        0x00000001028ca514 cxxbridge1$str$ptr + 2690020\n14  libsystem_pthread.dylib             0x0000000195449c0c _pthread_start + 136\n15  libsystem_pthread.dylib             0x0000000195444b80 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Wait for the reviews to load\u001b[39;00m\n\u001b[1;32m     19\u001b[0m wait \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mvisibility_of_element_located((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview-container\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Extract and print all review texts\u001b[39;00m\n\u001b[1;32m     23\u001b[0m review_elements \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext.show-more__control\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:138\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n0   chromedriver                        0x00000001028d2654 cxxbridge1$str$ptr + 2723108\n1   chromedriver                        0x00000001028ca8c8 cxxbridge1$str$ptr + 2690968\n2   chromedriver                        0x000000010241e714 cxxbridge1$string$len + 90428\n3   chromedriver                        0x00000001024657c0 cxxbridge1$string$len + 381416\n4   chromedriver                        0x00000001024a6de8 cxxbridge1$string$len + 649232\n5   chromedriver                        0x00000001024599c8 cxxbridge1$string$len + 332784\n6   chromedriver                        0x0000000102896278 cxxbridge1$str$ptr + 2476360\n7   chromedriver                        0x000000010289950c cxxbridge1$str$ptr + 2489308\n8   chromedriver                        0x0000000102877a64 cxxbridge1$str$ptr + 2351412\n9   chromedriver                        0x0000000102899d94 cxxbridge1$str$ptr + 2491492\n10  chromedriver                        0x0000000102868d58 cxxbridge1$str$ptr + 2290728\n11  chromedriver                        0x00000001028b9d60 cxxbridge1$str$ptr + 2622512\n12  chromedriver                        0x00000001028b9eec cxxbridge1$str$ptr + 2622908\n13  chromedriver                        0x00000001028ca514 cxxbridge1$str$ptr + 2690020\n14  libsystem_pthread.dylib             0x0000000195449c0c _pthread_start + 136\n15  libsystem_pthread.dylib             0x0000000195444b80 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "#Reviews in page 1\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# IMDb URL\n",
    "url = 'https://www.imdb.com/title/tt4154796/reviews?spoiler=hide&sort=curated&dir=desc&ratingFilter=0'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the reviews to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'lister-list')))\n",
    "\n",
    "# Extract and print all review texts\n",
    "review_elements = driver.find_elements(By.CLASS_NAME, 'text.show-more__control')\n",
    "for i, review_element in enumerate(review_elements, start=1):\n",
    "    print(f\"Review {i}: {review_element.text.strip()}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDxBQ4V1bj02"
   },
   "outputs": [],
   "source": [
    "#all reviews and ratings\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# IMDb URL for Avengers: Endgame reviews\n",
    "url = 'https://www.imdb.com/title/tt4154796/reviews?spoiler=hide&sort=curated&dir=desc&ratingFilter=0'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the reviews to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'lister-list')))\n",
    "\n",
    "# Function to click \"Load More\" button until all reviews are loaded\n",
    "def load_all_reviews():\n",
    "    while True:\n",
    "        try:\n",
    "            load_more_button = driver.find_element(By.CLASS_NAME, 'ipl-load-more__button')\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "            load_more_button.click()\n",
    "            wait.until(EC.invisibility_of_element_located((By.CLASS_NAME, 'ipl-load-more__load-indicator')))\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "# Function to extract ratings and reviews\n",
    "def extract_data():\n",
    "    rating_elements = driver.find_elements(By.CLASS_NAME, 'ipl-ratings-bar')\n",
    "    review_elements = driver.find_elements(By.CLASS_NAME, 'text')\n",
    "\n",
    "    data = []\n",
    "    for rating_element, review_element in zip(rating_elements, review_elements):\n",
    "        rating = rating_element.text.strip().split('\\n')[0]\n",
    "        review = review_element.text.strip()\n",
    "        data.append({'Rating': rating if rating else None, 'Review': review if review else None})\n",
    "        print(f\"Rating: {rating if rating else 'None'}\")\n",
    "        print(f\"Review: {review if review else 'None'}\\n\")\n",
    "    return data\n",
    "\n",
    "# Load all reviews and extract the ratings and reviews\n",
    "load_all_reviews()\n",
    "all_data = extract_data()\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dORh5YIzZ94-"
   },
   "outputs": [],
   "source": [
    "# Extract text content from review elements\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the ratings and reviews\n",
    "ratings_reviews_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save DataFrame to Excel with ratings and reviews in different columns\n",
    "output_file = \"/content/drive/MyDrive/Reviews.xlsx\"\n",
    "ratings_reviews_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5jOKjmpi9SA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "ratings_reviews_df = pd.read_excel(\"/content/drive/MyDrive/Reviews.xlsx\")\n",
    "\n",
    "# Group the data by the \"ratings\" column and count occurrences\n",
    "ratings_counts = ratings_reviews_df['Rating'].value_counts()\n",
    "\n",
    "# Sort the ratings_counts Series in descending order\n",
    "ratings_counts = ratings_counts.sort_values(ascending=False)\n",
    "\n",
    "# Define colors for each rating value\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan', 'brown','pink','navy','yellow']\n",
    "\n",
    "# Plot a bar graph\n",
    "ax = ratings_counts.plot(kind='bar', color=colors)\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Add number counts on each bar\n",
    "for i, count in enumerate(ratings_counts):\n",
    "    ax.text(i, count + 10, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU1HARCmn-qz"
   },
   "source": [
    "SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxuqDikan-3i"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN65036Kn-58"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlzEc9xXn-9Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the Excel file\n",
    "input_file = \"/content/drive/MyDrive/Reviews.xlsx\"\n",
    "ratings_reviews_df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to calculate sentiment score based on ratings\n",
    "def calculate_sentiment(rating):\n",
    "    # Extract the numerical part of the rating (e.g., \"7/10\" -> 7)\n",
    "    rating_value = float(rating.split('/')[0])\n",
    "    # Calculate sentiment score\n",
    "    sentiment_score = 1 if rating_value > 7 else 0\n",
    "    return sentiment_score\n",
    "\n",
    "# Apply the calculate_sentiment function to the 'Rating' column and add the result as a new column 'Sentiment'\n",
    "ratings_reviews_df['Sentiment'] = ratings_reviews_df['Rating'].apply(calculate_sentiment)\n",
    "\n",
    "# Save the updated DataFrame to Excel with the sentiment score added\n",
    "output_file = \"/content/drive/MyDrive/Reviews-1S.xlsx\"\n",
    "ratings_reviews_df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(ratings_reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohxBN89XoG_s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training = pd.read_excel(\"/content/drive/MyDrive/Reviews-1S.xlsx\", header=0)\n",
    "print(training.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Fstgu5FoJGE"
   },
   "outputs": [],
   "source": [
    "y_train = training['Sentiment']\n",
    "x_train = training.drop([\"Sentiment\",\"Rating\"], axis=1)\n",
    "\n",
    "print(x_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsJvGjX0oMN4"
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmlzVsruU9NF"
   },
   "source": [
    "CLEANING - DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqI3amtvVO5y"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#read file with reviews\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "\n",
    "df = pd.read_excel('/content/drive/MyDrive/Reviews-1S.xlsx')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4AxPN4LVdVr"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "##### Training Data\n",
    "import pandas as pd\n",
    "training= pd.read_excel('/content/drive/MyDrive/Reviews-1S.xlsx')\n",
    "print(training.shape)\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fkug5QfcVpmm"
   },
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import list of stopwords from library NLTK\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtUZmsIRVtl6"
   },
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "print(f'List of stopwords:\\n{stopwords_list}\\n')\n",
    "\n",
    "# We remove negation words in list of stopwords\n",
    "no_stopwords = [\"not\",\"don't\",'aren','don','ain',\"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
    "               'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "               \"won't\", 'wouldn', \"wouldn't\"]\n",
    "for no_stopword in no_stopwords:\n",
    "    stopwords_list.remove(no_stopword)\n",
    "\n",
    "print(f'Final list of stopwords:\\n{stopwords_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73x_5zM6VumK"
   },
   "outputs": [],
   "source": [
    "# Lemmatize reviews\n",
    "\n",
    "# Import Lemmatizer from NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function that receive a list of words and do lemmatization:\n",
    "def lemma_stem_text(words_list):\n",
    "    # Lemmatizer\n",
    "    text = [lemmatizer.lemmatize(token.lower()) for token in words_list] # eighties->eight or messages->message or drugs->drug\n",
    "    text = [lemmatizer.lemmatize(token.lower(), \"v\") for token in text ]# going-> go or started->start or watching->watch\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wp2uU0JNVxbz"
   },
   "outputs": [],
   "source": [
    "#Negative Contractions\n",
    "\n",
    "# create a function to change negative contractions (e.g. isn't, can't, ... etc.) to standard from using a regular expression.\n",
    "import re\n",
    "re_negation = re.compile(\"n't \") # specify a pattern you want to find in a string\n",
    "\n",
    "# function that receive a sequence of words and return the same sequence transforming\n",
    "# abbreviated negations to the standard form.\n",
    "def negation_abbreviated_to_standard(sent):\n",
    "    sent = re_negation.sub(\" not \", sent)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRq0FqyuV0PZ",
    "outputId": "a1142193-e0e4-450e-bea5-0e3a73a8a754"
   },
   "outputs": [],
   "source": [
    "#REVIEW\n",
    "\n",
    "# create a function to clean the text of a review using the functions defined previously.\n",
    "\n",
    "# Import function BeautifulSoup to clean text of HTML tags\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    # 1. Remove HTML tags\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "\n",
    "    # 2. Transform abbreviated negations to the standard form.\n",
    "    review_text = negation_abbreviated_to_standard(review_text)\n",
    "\n",
    "    # 3. Remove non-letters and non-numbers\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "\n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if w.lower() not in stopwords_list]\n",
    "\n",
    "    # 5. Apply lemmatization function\n",
    "    lemma_words = lemma_stem_text(meaningful_words)\n",
    "\n",
    "    # 6. Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join(lemma_words))\n",
    "\n",
    "    # 7. Handling Short Words\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    # 8. Handling Rare Words\n",
    "    word_counts = Counter(tokens)\n",
    "    common_words = set(word for word, count in word_counts.items() if count > 5)\n",
    "    tokens = [word for word in tokens if word in common_words]\n",
    "\n",
    "    # 9. # Drop rows with NaN values in the 'Reviews' column\n",
    "    training.dropna(subset=['Review'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKdD-8H0V3Lu"
   },
   "outputs": [],
   "source": [
    "# Clean the first review\n",
    "clean_review = review_to_words(training[\"Review\"].iloc[0])\n",
    "\n",
    "# Print original review, and cleaned review\n",
    "print(f'Text of original review:\\n{training[\"Review\"].iloc[0]}\\n')\n",
    "print(f'Text of cleaned review:\\n{clean_review}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n9SDQ4CV5yV"
   },
   "outputs": [],
   "source": [
    "# We get the text of reviews in the training set\n",
    "reviews = x_train['Review']\n",
    "\n",
    "# We initialize an empty list to add the clean reviews\n",
    "cleaned_train_reviews = []\n",
    "\n",
    "# We loop over each review and clean it\n",
    "for i in reviews:\n",
    "    cleaned_train_reviews.append(review_to_words(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDBx_-AeV8WV"
   },
   "outputs": [],
   "source": [
    "# Print ALL cleaned reviews\n",
    "print(cleaned_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5N_du-brdJo"
   },
   "outputs": [],
   "source": [
    "#wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Join all cleaned reviews into a single string\n",
    "all_reviews_text = ' '.join(cleaned_train_reviews)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_reviews_text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZUGldJpWj3D"
   },
   "source": [
    "DOCUMENT TERM MATRIX- (DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rinI0U43o2K4"
   },
   "source": [
    "Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4XV9cKBWUmd"
   },
   "outputs": [],
   "source": [
    "# Import Count Vectorizer encoding from sklearn library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "c = CountVectorizer(stop_words='english', lowercase=True, token_pattern=r'\\w+', ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform y_train to generate the Bigram DTM\n",
    "train_data_features_d = c.fit_transform(cleaned_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDweE_UVWUoo"
   },
   "outputs": [],
   "source": [
    "# We split train dataset to create validation set and training the Logistic Regression Model on validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_d, X_val_d, Y_train_d, Y_val_d = train_test_split(train_data_features_d, y_train, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRsMDjWNWUsE"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize a logistic regression model\n",
    "logistic_d = LogisticRegression(random_state=0)\n",
    "# Train the model\n",
    "logistic_d.fit(X_train_d, Y_train_d)\n",
    "\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = logistic_d.predict(X_val_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3o13nCvoWy7T"
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_pred_logistic_d  = logistic_d.predict(X_val_d)\n",
    "confusion_matrix_logistic_d = confusion_matrix(Y_val_d, Y_pred_logistic_d, labels=[1,0])\n",
    "confusion_matrix_logistic_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2--ICmsW1AP"
   },
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(confusion_matrix_logistic_d, annot=True, fmt=\"d\")\n",
    "ax.set_title('Confusion matrix Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Lz6zybJHyPm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score,roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Calculate accuracy and round to 2 decimal places\n",
    "accuracy = round(accuracy_score(Y_val_d, y_pred), 2)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_d = logistic_d.predict(X_val_d)\n",
    "\n",
    "# Calculate precision\n",
    "precision_d = precision_score(Y_val_d, y_pred_d)\n",
    "\n",
    "# Calculate recall\n",
    "recall_d = recall_score(Y_val_d, y_pred_d)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_d = f1_score(Y_val_d, y_pred_d)\n",
    "\n",
    "\n",
    "print(\"Precision (DTM):\", precision_d)\n",
    "print(\"Recall (DTM):\", recall_d)\n",
    "print(\"F1 Score (DTM):\", f1_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK_1JxO7pKLk"
   },
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KSbBxs5p7Bu"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Initialize SVM model\n",
    "svm_model = SVC(random_state=0)\n",
    "\n",
    "#Train the SVM model\n",
    "svm_model.fit(X_train_d, Y_train_d)\n",
    "\n",
    "#Predict on the validation set\n",
    "Y_pred_svm_d = svm_model.predict(X_val_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijONhZAtqOPR"
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy for SVM model\n",
    "accuracy_svm = accuracy_score(Y_val_d, Y_pred_svm_d)\n",
    "precision_svm = precision_score(Y_val_d, Y_pred_svm_d)\n",
    "recall_svm = recall_score(Y_val_d, Y_pred_svm_d)\n",
    "f1_svm = f1_score(Y_val_d, Y_pred_svm_d)\n",
    "\n",
    "print(\"\\nSVM Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_svm)\n",
    "print(\"Precision:\", precision_svm)\n",
    "print(\"Recall:\", recall_svm)\n",
    "print(\"F1 Score:\", f1_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfyfgcAfshnw"
   },
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUcN7MaVsk7w"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Train the Random Forest model\n",
    "random_forest_model.fit(X_train_d, Y_train_d)\n",
    "\n",
    "# Predict on the validation set\n",
    "Y_pred_rf_d = random_forest_model.predict(X_val_d)\n",
    "\n",
    "# Calculate metrics for Random Forest model\n",
    "accuracy_rf = accuracy_score(Y_val_d, Y_pred_rf_d)\n",
    "precision_rf = precision_score(Y_val_d, Y_pred_rf_d)\n",
    "recall_rf = recall_score(Y_val_d, Y_pred_rf_d)\n",
    "f1_rf = f1_score(Y_val_d, Y_pred_rf_d)\n",
    "\n",
    "print(\"Random Forest Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Precision:\", precision_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "print(\"F1 Score:\", f1_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKO_tBAVW3ni"
   },
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqmZy71xW5Ma"
   },
   "outputs": [],
   "source": [
    "# Import tf-idf encoding from sklearn library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define some hyperparameters of encoded\n",
    "vectorizer = TfidfVectorizer(min_df=0.5, max_df=0.90,ngram_range = (1,2))\n",
    "\n",
    "# Create the training set with the words encoded as features of the reviews\n",
    "train_data_features_t = vectorizer.fit_transform(cleaned_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmPVLQT6W5Ph"
   },
   "outputs": [],
   "source": [
    "# We split train dataset to create validation set and training the Logistic Regression Model on validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_t, X_val_t, Y_train_t, Y_val_t = train_test_split(train_data_features_t, y_train, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMEdhdTlW5S6"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a logistic regression model\n",
    "logistic_t = LogisticRegression(random_state=0)\n",
    "# Train the model\n",
    "logistic_t = logistic_t.fit(X_train_t, Y_train_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vC37x8tDXA7j"
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_pred_logistic_t  = logistic_t.predict(X_val_t)\n",
    "confusion_matrix_logistic_t = confusion_matrix(Y_val_t, Y_pred_logistic_t, labels=[1,0])\n",
    "confusion_matrix_logistic_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Do-aRHQRXBi2"
   },
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(confusion_matrix_logistic_t, annot=True, fmt=\"d\")\n",
    "ax.set_title('Confusion matrix Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5J4X9QBIj5c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_t = logistic_t.predict(X_val_t)\n",
    "\n",
    "# Calculate metrics for logistic regression model\n",
    "accuracy_logistic_t = accuracy_score(Y_val_t, y_pred_t)\n",
    "precision_logistic_t = precision_score(Y_val_t, y_pred_t)\n",
    "recall_logistic_t = recall_score(Y_val_t, y_pred_t)\n",
    "f1_logistic_t = f1_score(Y_val_t, y_pred_t)\n",
    "\n",
    "print(\"Logistic Regression Model Metrics on TF-IDF encoded data:\")\n",
    "print(\"Accuracy:\", accuracy_logistic_t)\n",
    "print(\"Precision:\", precision_logistic_t)\n",
    "print(\"Recall:\", recall_logistic_t)\n",
    "print(\"F1 Score:\", f1_logistic_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oYNI2oJo9gv"
   },
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA0vxS3Uo82G"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model_tfidf = SVC(random_state=0)\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model_tfidf.fit(X_train_t, Y_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDx1RckYqiTQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_svm_tfidf = svm_model_tfidf.predict(X_val_t)\n",
    "\n",
    "# Calculate metrics for SVM model\n",
    "accuracy_svm_tfidf = accuracy_score(Y_val_t, y_pred_svm_tfidf)\n",
    "precision_svm_tfidf = precision_score(Y_val_t, y_pred_svm_tfidf)\n",
    "recall_svm_tfidf = recall_score(Y_val_t, y_pred_svm_tfidf)\n",
    "f1_svm_tfidf = f1_score(Y_val_t, y_pred_svm_tfidf)\n",
    "\n",
    "print(\"SVM Model Metrics on TF-IDF encoded data:\")\n",
    "print(\"Accuracy:\", accuracy_svm_tfidf)\n",
    "print(\"Precision:\", precision_svm_tfidf)\n",
    "print(\"Recall:\", recall_svm_tfidf)\n",
    "print(\"F1 Score:\", f1_svm_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWgO4dFprUf3"
   },
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AnENyxfrSbn"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model.fit(X_train_t, Y_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxdGN9U_rrPL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_rf_t = rf_model.predict(X_val_t)\n",
    "\n",
    "# Calculate metrics for Random Forest model\n",
    "accuracy_rf_t = accuracy_score(Y_val_t, y_pred_rf_t)\n",
    "precision_rf_t = precision_score(Y_val_t, y_pred_rf_t)\n",
    "recall_rf_t = recall_score(Y_val_t, y_pred_rf_t)\n",
    "f1_rf_t = f1_score(Y_val_t, y_pred_rf_t)\n",
    "\n",
    "print(\"Random Forest Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_rf_t)\n",
    "print(\"Precision:\", precision_rf_t)\n",
    "print(\"Recall:\", recall_rf_t)\n",
    "print(\"F1 Score:\", f1_rf_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDQZd-crXGHA"
   },
   "source": [
    "WORD EMBEDDINGS - (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnDZGyLw4JhF"
   },
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJVwhNs8asDD"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Tokenize the cleaned reviews\n",
    "tokenized_reviews = [review.split() for review in cleaned_train_reviews]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = gensim.models.Word2Vec(\n",
    "    tokenized_reviews,\n",
    "    vector_size=150,\n",
    "    window=10,\n",
    "    min_count=3,\n",
    "    workers=10\n",
    ")\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "model.train(tokenized_reviews, total_examples=len(tokenized_reviews), epochs=10)\n",
    "\n",
    "# Save the trained word vectors\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"/content/drive/MyDrive/STAT653/my_w2v.kv\")\n",
    "\n",
    "# Load the saved word vectors\n",
    "loaded_word_vectors = gensim.models.KeyedVectors.load(\"/content/drive/MyDrive/STAT653/my_w2v.kv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WYIpKwzbJKg"
   },
   "outputs": [],
   "source": [
    "w1 = \"thanos\"\n",
    "loaded_word_vectors.most_similar (positive=w1,topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcHAbxPubfm4"
   },
   "outputs": [],
   "source": [
    "w2 = \"love\"\n",
    "loaded_word_vectors.most_similar (positive=w2, topn=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zeq7EIJhcKfU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtJ1vZ4O4RUc"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize the cleaned reviews\n",
    "tokenized_reviews = [review.split() for review in cleaned_train_reviews]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the Word2Vec model\n",
    "word2vec_model.save(\"/content/drive/MyDrive/word2vec_model.model\")\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"/content/drive/MyDrive/word2vec_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VXzAKJU4ZDj"
   },
   "outputs": [],
   "source": [
    "similar_words = word2vec_model.wv.most_similar('great', topn=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWQXpDrI6CsP"
   },
   "outputs": [],
   "source": [
    "similar_words = word2vec_model.wv.most_similar('thanos', topn=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OcfTTyo65KQ"
   },
   "outputs": [],
   "source": [
    "w1 = \"love\"\n",
    "similar_words = word2vec_model.wv.most_similar(positive=w1, topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CXfvMSe7S1h"
   },
   "outputs": [],
   "source": [
    "similarity_score = word2vec_model.wv.similarity(w1=\"love\", w2=\"3000\")\n",
    "\n",
    "# Print the similarity score\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kh3cKnwg7iNU"
   },
   "outputs": [],
   "source": [
    "# Find the odd word out\n",
    "odd_one_out = word2vec_model.wv.doesnt_match([\"thor\", \"thanos\", \"captain\"])\n",
    "\n",
    "# Print the odd word out\n",
    "print(odd_one_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajMXECb179VZ"
   },
   "outputs": [],
   "source": [
    "# Find the odd word out\n",
    "odd_one_out = word2vec_model.wv.doesnt_match([\"film\", \"thor\", \"ironman\"])\n",
    "\n",
    "# Print the odd word out\n",
    "print(odd_one_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nYpbnLFSL_p"
   },
   "source": [
    "FASTTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzvKJFNokoAs"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import FastText\n",
    "import pandas as pd\n",
    "\n",
    "tokenized_reviews = [review.split() for review in cleaned_train_reviews]\n",
    "phrases = Phrases(tokenized_reviews, min_count = 30, progress_per = 10000)\n",
    "sentences = phrases[tokenized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi2-w1Crk8hW"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = FastText(vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Building Vocabulary\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "# Get the keys using the .index_to_key method\n",
    "keys = model.wv.index_to_key\n",
    "\n",
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgdQy7anlGhj"
   },
   "outputs": [],
   "source": [
    "#Training the model\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "# Save the trained model using joblib\n",
    "import joblib\n",
    "joblib.dump(model, '/content/drive/MyDrive/STAT653/fasttext_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KReHSwK6lLlj"
   },
   "outputs": [],
   "source": [
    "vocabulary = model.wv.index_to_key\n",
    "print('uniform' in vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8-oNQaZoU-T"
   },
   "outputs": [],
   "source": [
    "vocabulary = model.wv.index_to_key\n",
    "print('3000' in vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igbOKbNVl0l5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cDwmTREl0t6"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Tokenize your reviews data\n",
    "tokenized_reviews = [review.split() for review in cleaned_train_reviews]\n",
    "\n",
    "# Train the FastText model\n",
    "model = FastText(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"/content/drive/MyDrive/fasttext_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD_1WGkzSPxw"
   },
   "outputs": [],
   "source": [
    "# Calculate similarity between two words\n",
    "similarity = model.wv.similarity(\"great\", \"amazing\")\n",
    "print(\"Similarity between 'great' and 'amazing':\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7aDsaVaSRoc"
   },
   "outputs": [],
   "source": [
    "# Calculate similarity between two words\n",
    "similarity = model.wv.similarity(\"action\", \"adventure\")\n",
    "print(\"Similarity between 'action' and 'adventure':\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHgtFeOqSVnR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StOI3CKwSVpk"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icOQ2H4eSVsA"
   },
   "outputs": [],
   "source": [
    "# Define the path for the output file\n",
    "output_file = '/content/drive/MyDrive/cleaned_train_reviews.txt'\n",
    "\n",
    "# Write the cleaned reviews to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write('\\n'.join(cleaned_train_reviews))\n",
    "\n",
    "print(\"Cleaned train reviews have been saved in:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THKWYNyASVth"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import time\n",
    "start_time = time.time()\n",
    "path = '/content/drive/MyDrive/cleaned_train_reviews.txt'\n",
    "model = fasttext.train_unsupervised(path , model='skipgram')\n",
    "end_time = time.time()\n",
    "print(f\"training time took {round(end_time - start_time, 3)} secs\")\n",
    "print(model.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIHtuxjoSVw3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity([model['performance']], [model['beautiful']]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
